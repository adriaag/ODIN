{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Embarked': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Embarked'), 'Survived': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Survived'), 'PassengerId': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/PassengerId'), 'SibSp': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/SibSp'), 'Name': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Name'), 'Age': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Age'), 'Pclass': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Pclass'), 'Cabin': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Cabin'), 'Ticket': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Ticket'), 'Sex': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Sex'), 'Fare': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Fare'), 'Parch': rdflib.term.URIRef('https://extremexp.eu/ontology/abox#titanic.csv/Parch')}\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from rdflib.term import Node\n",
    "\n",
    "# sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.abspath(os.path.join('..', '..'))))\n",
    "from pipeline_generator.optimized_pipeline_generator import *\n",
    "\n",
    "\n",
    "\n",
    "def abstract_planner(ontology: Graph, intent: Graph) -> Tuple[\n",
    "    Dict[Node, Dict[Node, List[Node]]], Dict[Node, List[Node]]]:\n",
    "    dataset, task, algorithm, intent_iri = get_intent_info(intent)\n",
    "\n",
    "\n",
    "    dataset_uri = URIRef(dataset)\n",
    "\n",
    "    cat_columns = {n.fragment.split(\"/\")[1]:\n",
    "                   n for n in set(set(ontology.objects(dataset_uri, dmop.hasColumn)).intersection(set(ontology.subjects(dmop.isCategorical, True)))\n",
    "                   #.intersection())\n",
    "                   }\n",
    "    \n",
    "    print(cat_columns)\n",
    "\n",
    "    # algs = get_algorithms_from_task(ontology, task)\n",
    "\n",
    "    # impls = get_potential_implementations(ontology, task, algorithm)\n",
    "\n",
    "    # algs_shapes = {}\n",
    "    # alg_plans = {alg: [] for alg in algs}\n",
    "    # for impl in impls:\n",
    "    #     alg = next(ontology.objects(impl[0], tb.implements)), \n",
    "    #     (impl[0], RDF.type, tb.Implementation) in ontology and (tb.ApplierImplementation not in ontology.objects(impl[0], RDF.type))\n",
    "\n",
    "    #     algs_shapes[alg[0]] = impl[1::][0][0]\n",
    "\n",
    "    #     alg_plans[alg[0]].append(impl)\n",
    "\n",
    "    # print(algs_shapes)\n",
    "\n",
    "    # act_dataset = ab.term(dataset)\n",
    "    # dataset_columns = {n.fragment: n for n in ontology.subjects(RDF.type, dmop.Column) if (act_dataset, dmop.hasColumn, n) in ontology}\n",
    "    # print(f'COLUMNS: {dataset_columns}')\n",
    "    \n",
    "    # # for impl in impls:\n",
    "\n",
    "    #     # key = next(ontology.objects(impl[0], tb.implements))\n",
    "        \n",
    "    #     # alg_plans[key].append(impl)\n",
    "\n",
    "\n",
    "    # # train=True\n",
    "    # plans = {}\n",
    "    # for alg in algs:\n",
    "    #     if cb.TrainTabularDatasetShape in algs_shapes[alg]:\n",
    "    #         trainer = cb.term(alg.fragment + '-Train')\n",
    "    #         plans[alg] = {\n",
    "    #             cb.DataLoading: [cb.TrainTestSplit],\n",
    "    #             cb.TrainTestSplit: [trainer, alg],\n",
    "    #             trainer: [alg],\n",
    "    #             alg: [cb.DataStoring],\n",
    "    #             cb.DataStoring: []\n",
    "    #         }\n",
    "    #     else:\n",
    "    #         plans[alg] = {\n",
    "    #             cb.DataLoading: [alg],\n",
    "    #             alg: [],\n",
    "    #             # cb.DataStoring: []\n",
    "    #         }\n",
    "    #     # plans[alg] = {\n",
    "    #     #     cb.DataLoading: [alg],\n",
    "    #     #     alg: [],\n",
    "    #     #     # cb.DataStoring: []\n",
    "    #     # }\n",
    "\n",
    "    # return plans, alg_plans\n",
    "\n",
    "\n",
    "ontology = get_ontology_graph()\n",
    "intent_graph = get_graph_xp()\n",
    "\n",
    "intent_name = 'ci'\n",
    "\n",
    "intent_graph.add((ab.term(intent_name), RDF.type, tb.Intent))\n",
    "intent_graph.add((ab.term(intent_name), tb.overData, ab.term('titanic.csv')))\n",
    "intent_graph.add((cb.Classification, tb.tackles, ab.term(intent_name)))\n",
    "\n",
    "intent = intent_graph\n",
    "\n",
    "abstract_planner(ontology, intent)\n",
    "# print('---------------------------------------------------')\n",
    "\n",
    "# print(f'ABS PLANS: {abs_plans}')\n",
    "# print(f'ALG PLANS:{alg_plans}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
